
# 并发导入的控制，请和后台对象存储support 沟通，提高IOPS
IMPORT_CONCURRENCY=5
# 并发导出的初始并发度,【后续会根据源集群的内存，cpu, io latency 等指标，进行动态控制】
EXPORT_CONCURRENCY=1

# 导出数据的目标位置,用逗号隔开多个，第一个默认做为源集群数据导出的，最后一个默认是目标集群导入的数据文件的存储位置。如果是多个对象存储之间，则需要使用DTH 进行同步
STORAGES=s3://tx-au-mock-data/p202503/
# 如果使用专线，需要配置vpc interface endpoint, endpoint的安全组需要容许源机器的IP的流量写入
S3_INTERFACE_ENDPOINT=vpce-06bd47f9238d6dc42-swt2xlgp.s3.eu-central-1.vpce.amazonaws.com
TASK_QUEUE=https://sqs.eu-central-1.amazonaws.com/515491257789/starrocks_migration_queue
# dynamodb table name for record the event action
RECORDER=starrocks-migration-task-trace-event

# REGION
AWS_REGION=eu-central-1
# 如果所在的执行环境没有绑定拥有权限的的角色，则需要配置AK, SK
AK=ak
SK=sk

# 数据来源的STARROCKS 集群，user 需要具有对源集群目标数据库表的数据读取权限，Schema读取权限
# FE 地址,可以有多个,用逗号隔开
SOURCE_HOST=172.31.4.217
SOURCE_PORT=9030
SOURCE_USER=root
SOURCE_PWD=demo1234
SOURCE_DB_NAME=sungrow

# 目标STARROCKS 集群
# FE 地址
TARGET_HOST=172.31.4.217
TARGET_PORT=9030
TARGET_USER=root
TARGET_PWD=demo1234
TARGET_DB_NAME=sunex

# 要导出的表集合,多个表用逗号隔开
TABLE_NAME=fact_organization_kpi_year4

# 分区过滤条件
# 格式 method(pt1,pt2)
# method可以有如下类型：EACH,STARTWITH,ENDWITH,RANGE
# 如果不需要过滤请留空
# 例如 EACH(p20251001,p20251005,p20251111) ，则只会同步 这三个分区
# 例如 RANGE(p20251001,p20251005) ，则只会同步p20251001,p20251005 直接的所有分区，不包括p20251005
# 例如 STARTWITH(p20251001) ，则只会同步p20251001 之后的所有分区，包括p20251001
TASK_FILTER=

# spark exporter相关配置
DEPENDENCY_JARS=/home/ec2-user/lib/hadoop-aws-3.3.4.jar,/home/ec2-user/lib/aws-java-sdk-bundle-1.12.367.jar,/home/ec2-user/lib/starrocks-spark-connector-3.4_2.12-1.1.2.jar,/home/ec2-user/lib/mysql-connector-java-8.0.22.jar
PER_FILE_MAX_ROW_COUNT=100000